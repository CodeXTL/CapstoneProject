#Imports 
import numpy as np
import matplotlib.pyplot as plt
import h5py
import scipy
from PIL import Image
from scipy import ndimage
from utils import load_dataset
import time

# Loading data from h5py files
train_set_x_orig, train_set_y, test_set_x_orig, test_set_y, classes = load_dataset()

#Flattens image data, each column is a single image (stacked by column)
#Transform train/test sets 3D arrays (layers of matrices) into 2D arrays (array of vectors)
#The "T" means transposed, matrix is reflected across major diagonal
train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T
test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T

#Standardize dataset
train_set_x = train_set_x_flatten/255
test_set_x = test_set_x_flatten/255

def showDim():
    """
    Method that shows the dimensions of the train/test set
    """
    m_train = train_set_x_orig.shape[0]
    m_test = test_set_x_orig.shape[0]
    num_px = train_set_x_orig.shape[1]

    print("train_set_x shape: " + str(train_set_x_orig.shape))
    print("train_set_y shape: " + str(train_set_y.shape))
    print("test_set_x shape: " + str(test_set_x_orig.shape))
    print("test_set_y shape: " + str(test_set_y.shape))
    print()
    print("Number of training examples: m_train = " + str(m_train))
    print("Number of testing examples: m_test = " + str(m_test))
    print("Height/Width of each image: num_px = " + str(num_px))
    print("Each image is of size: (" + str(num_px) + ", " + str(num_px) + ", 3)")
    print()
    print ("train_set_x_flatten shape: " + str(train_set_x_flatten.shape))
    print ("train_set_y shape: " + str(train_set_y.shape))
    print ("test_set_x_flatten shape: " + str(test_set_x_flatten.shape))
    print ("test_set_y shape: " + str(test_set_y.shape))
    print ("sanity check after reshaping: " + str(train_set_x_flatten[0:5,0]))
    print()
    print("train_set_x shape:", train_set_x.shape)
    print("train_set_y shape:", train_set_y.shape)
    print("test_set_x shape:", test_set_x.shape)
    print("test_set_y shape:", test_set_y.shape)

def sigmoid(z):
    """
    Computer the sigmoid of z

    Arguments:\n
    z -- A scalar or numpy array of any size\n

    Return:\n
    s -- sigmoid(z)\n
    """
    s = 1/(1 + np.exp(-z))
    
    return s

def layer_sizes(X, Y, h):
    """
    Argument:\n
    X -- input dataset of shape (input/image size, number of examples)\n
    Y -- labels of shape (output size, number of examples)\n
    h -- number of nodes in the hidden layer

    Returns:\n
    Tuple containing the following\n
    n_x -- size of the input layer\n
    n_h -- size of the hidden layer\n
    n_y -- size of the output layer\n
    """
    n_x = X.shape[0]
    n_h = h
    n_y = Y.shape[0]

    return (n_x, n_h, n_y)

def initialize_parameters(n_x, n_h, n_y):
    """
    Initializes parameters for a two layer neural network

    Arguments:\n
    n_x -- size of the input layer\n
    n_h -- size of the hidden layer\n
    n_y -- size of the output layer\n

    Returns:\n
    params -- python dictionary containing the parameters for each layer\n
    W1 -- weight matrix of shape (n_h, n_x)\n
    b1 -- bias vector of shape (n_h, 1)\n
    W2 -- weight matrix of shape (n_y, n_h)\n
    b2 -- bias vector of shape (n_y, 1)\n
    """
    W1 = np.random.randn(n_h, n_x) * 0.01   #Smaller weights are better/stabler
    b1 = np.zeros((n_h, 1))
    W2 = np.random.randn(n_y, n_h) * 0.01
    b2 = np.zeros((n_y, 1))

    params = {"W1": W1,
              "b1": b1,
              "W2": W2,
              "b2": b2}
    
    return params

def forward_propagation(X, params):
    """
    Calculate the final score for a test example (ranges from 0-1)
    This method also functions as the predict method

    Arguments:\n
    X -- input date of size (n_x, m)\n
    params -- python dictionary\n
    
    Returns:\n
    A2 -- The final computed value to compare with the Y set, returned with cache because it's useful
    cache -- a dictionary containing "Z1", "A1", "Z2", and "A2"\n
    """
    W1 = params['W1']
    b1 = params['b1']
    W2 = params['W2']
    b2 = params['b2']

    Z1 = np.dot(W1, X) + b1
    A1 = np.tanh(Z1)
    Z2 = np.dot(W2, A1) + b2
    A2 = sigmoid(Z2)

    cache = {'Z1': Z1,
             'A1': A1,
             'Z2': Z2,
             'A2': A2}

    return A2, cache

def compute_cost(A2, Y):
    """
    Computes cross-entropy cost

    Arguments:\n
    A2 -- The sigmoid output of the second activation, of shape (1, m)\n
    Y -- "true" labels for cat images, of shape (1, m)\n

    Returns:\n
    cost -- cross-entropy cost\n
    """
    m = Y.shape[1]  #m = number of examples
    probs = np.multiply(Y, np.log(A2)) + np.multiply(1-Y, np.log(1-A2))
    cost = -1/m * np.sum(probs)

    cost = float(np.squeeze(cost))  #Ensures that cost is a single number of float type

    return cost

def backward_propagation(params, cache, X, Y):
    """
    Implement backward propagation for 2 layered nn

    Arguements:\n
    parameters -- python dictionary containing our parameters\n
    cache -- a dictionary containing "Z1", "A1", "Z2", and "A2".\n
    X -- input data of shape (64*64*3, m)\n
    Y -- input data of shape (1, m)\n

    Returns:\n
    grads -- python dictionary containing the gradients for both layers\n
    """
    m = X.shape[1]
    W1 = params['W1']
    b1 = params['b1']
    W2 = params['W2']
    b2 = params['b2']

    A1 = cache['A1']
    A2 = cache['A2']

    dZ2 = A2 - Y
    dW2 = 1/m * np.dot(dZ2, A1.T)
    db2 = 1/m * np.sum(dZ2, axis=1, keepdims=True)
    dZ1 = np.dot(W2.T, dZ2) * (1 - np.power(A1, 2))
    dW1 = 1/m * np.dot(dZ1, X.T)
    db1 = 1/m * np.sum(dZ1, axis=1, keepdims=True)

    grads = {'dW1': dW1,
             'db1': db1,
             'dW2': dW2,
             'db2': db2}

    return grads

def update_params(params, grads, lr):
    """
    Updates parameters using the gradient descent

    Arguements:\n
    params -- python dictionary contaning the parameters (from forward propagation)\n
    grads -- python dictionary containing the gradients (from backward propagation)\n
    lr -- learning rate

    Returns:\n
    params -- dictionary containing the updated parameters\n
    """
    #The params
    W1 = params['W1']
    b1 = params['b1']
    W2 = params['W2']
    b2 = params['b2']

    #The gradients/derivatives
    dW1 = grads['dW1']
    db1 = grads['db1']
    dW2 = grads['dW2']
    db2 = grads['db2']

    #Calculate new params, kind of like tangent-line approximation
    #In simple terms, the params are changed in the direction
    #that force the cost to go down the slope.
    W1 = W1 - lr * dW1
    b1 = b1 - lr * db1
    W2 = W2 - lr * dW2
    b2 = b2 - lr * db2

    #Update the params
    params = {'W1': W1,
              'b1': b1,
              'W2': W2,
              'b2': b2}

    return params

def gradient_descent(params, X, Y, lr, num_iter, print_cost=False):
    costs = []
    for i in range(0, num_iter):
        #Forward propagation
        A2, cache = forward_propagation(X, params)
        #Cost function
        cost = compute_cost(A2, Y)
        #Backward propagation
        grads = backward_propagation(params, cache, X, Y)
        #Update params
        params = update_params(params, grads, lr)
        #Print the cost for every 1000 iterations if print_cost=True
        if print_cost and (i % 1000 == 0 or i == num_iter-1):
            print("Cost after iteration", i, ":", cost)
            costs.append(cost)

    return costs, params

def predict(X, params):
    A2, unused_cache = forward_propagation(X, params)
    Y_prediction = (A2 > 0.5) * 1.0

    return Y_prediction

def nn_model(X, Y, n_h, lr, num_iter = 10000, print_cost=False):
    """
    A single method that combines all the previous methods
    into a single model

    Arguments:\n
    X -- input set X stacked in columns, shape (64*64*3, m)
    Y -- output set Y for checking, shape (1, m)
    n_h -- number of layers (excluding the first input layer)
    lr -- learning rate, rate at which params change
    num_iter -- number of iterations, default = 10000
    print_cost -- bool that determines if cost will be printed, default = False
    
    Return:\n
    costs -- list of all costs
    params -- the final params
    """
    n_x = layer_sizes(X, Y, n_h)[0]
    n_y = layer_sizes(X, Y, n_h)[2]

    #Initialize params
    params = initialize_parameters(n_x, n_h, n_y)

    #Gradient descent
    costs, params = gradient_descent(params, X, Y, lr, num_iter, print_cost)



    #Return the best params for the model
    return costs, params

#Training the model
start_time = time.time()
costs, final_params = nn_model(train_set_x, train_set_y, 2, 0.01, num_iter=10000,print_cost=True)
end_time = time.time()
print("Learning Time:", end_time - start_time)

#Show cost plot, lower cost = better
plt.plot(costs)
plt.show()
